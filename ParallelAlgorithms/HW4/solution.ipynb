{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero instalamos lo necesario para poder ejecutar pyOpenCl en el entorno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "OiRQePdLjddY"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!sudo apt -y update\n",
    "!sudo apt install -y nvidia-cuda-toolkit\n",
    "!sudo apt install -y pocl-opencl-icd\n",
    "!pip install pyopencl\n",
    "!pip install siphash24"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora importaremos las librerías necesarias a lo largo de la tarea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "neMtDC8g3g9c"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow_datasets as tfds\n",
    "from matplotlib import pylab as plt\n",
    "import pyopencl as cl\n",
    "import pyopencl.array as cl_array\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Descargamos la base de datos, y vemos cuantos datos tenemos para verificar que se haya descargado correctamente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 118,
     "referenced_widgets": [
      "e939c1dc9f4648b78dcdaac48dc33fb0",
      "f6fc955f1436400db5e33ea64f5a0ffd",
      "80e40d97a7f141e8aca43a544c0fc3b3",
      "a6684a8c7093413480b76fb315972469",
      "774f65b2418e42fc841a672d1fe5b524",
      "cd3acb1a6cfa4d888de1f0c594a7016a",
      "522f58dee3fb4f87b8ae0dfd3b375a35",
      "09fe9bac33f74916a02d6d5129165473",
      "1cc0858e19184a5f8e9efeacd3afe000",
      "25ad33f379c24beb88cf56b6572d949d",
      "cbd8edc90543413ebd5014100cab8134"
     ]
    },
    "id": "wNr97tAZ3b8Y",
    "outputId": "d854f425-953e-405d-a246-eee11369f04b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset 11.06 MiB (download: 11.06 MiB, generated: 21.00 MiB, total: 32.06 MiB) to /root/tensorflow_datasets/mnist/3.0.1...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e939c1dc9f4648b78dcdaac48dc33fb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Completed...:   0%|          | 0/5 [00:00<?, ? file/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset mnist downloaded and prepared to /root/tensorflow_datasets/mnist/3.0.1. Subsequent calls will reuse this data.\n",
      "Número de elementos de datos de entrenamiento: 60000\n",
      "Numero de elementos de datos de testeo: 10000\n"
     ]
    }
   ],
   "source": [
    "(mnist_data_train, mnist_data_test), data_info = tfds.load(\"mnist\", split=[\"train\", \"test\"], with_info=True, download=True)\n",
    "print(\"Número de elementos de datos de entrenamiento:\", data_info.splits[\"train\"].num_examples)\n",
    "print(\"Numero de elementos de datos de testeo:\", data_info.splits[\"test\"].num_examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora vamos a preprocesar los datos. Para esto primero sacamos las imagenes y las etiquetas de la base de datos, y las etiquetas las transformamos en variables binarias. Después, como las imagenes son en dos dimensiones las aplanamos para que quede en una sola dimensión, y además dividimos en 255 para que los pixeles vayan entre 0 y 1. Finalmente imprimimos las dimensiones y el tipo de datos para verificar que todo se haya realizado correctamente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tn4kI36iKIrp",
    "outputId": "ebe4f03d-603a-4ae2-86bb-0da106641f47"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Los 60000 datos de entrenamiento tienen dimensión 784 y dtype float64\n",
      "Los 10000 datos de entrenamiento tienen dimensión 784 y dtype float64\n"
     ]
    }
   ],
   "source": [
    "labels_true = [0, 2, 4, 6, 8]\n",
    "labels_false = [1, 3, 5, 7, 9]\n",
    "\n",
    "train_dataset_images = [example['image'] for example in tfds.as_numpy(mnist_data_train)]\n",
    "train_dataset_labels = [example['label'] in labels_true for example in tfds.as_numpy(mnist_data_train)]\n",
    "\n",
    "test_dataset_images = [example['image'] for example in tfds.as_numpy(mnist_data_test)]\n",
    "test_dataset_labels = [example['label'] in labels_true for example in tfds.as_numpy(mnist_data_test)]\n",
    "\n",
    "train_dataset_images_vector = [image.flatten()/255 for image in train_dataset_images]\n",
    "test_dataset_images_vector = [image.flatten()/255 for image in test_dataset_images]\n",
    "\n",
    "train_features = np.vstack(train_dataset_images_vector).T\n",
    "train_labels = np.vstack(train_dataset_labels).T\n",
    "\n",
    "test_features = np.vstack(test_dataset_images_vector).T\n",
    "test_labels = np.vstack(test_dataset_labels).T\n",
    "\n",
    "dim, n_train = train_features.shape\n",
    "print(\"Los\", n_train, \"datos de entrenamiento tienen dimensión\", dim, \"y dtype\", train_features.dtype)\n",
    "\n",
    "dim, n_test = test_features.shape\n",
    "print(\"Los\", n_test, \"datos de entrenamiento tienen dimensión\", dim, \"y dtype\", test_features.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora vamos a agregar de inmediato los datos necesarios para que las bases de datos sean múltiplo del tamaño local. Lo hacemos de inmediato para que las comparaciones sean justas (me hace ruido que las benchmarks tengan distinta cantidad de datos). Para esto fijo que el tamaño local sea de 64 y calculo los tamaños totales para cada uno de los conjuntos de datos, esto lo hacemos buscando el primer múltiplo de 64 que sea mayor o igual que los datos que tenemos, con un código estándar para hacer esto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "-43_bvMWkGjB"
   },
   "outputs": [],
   "source": [
    "local_size = 64\n",
    "global_size_train = ((n_train + local_size - 1)//local_size)*local_size\n",
    "global_size_test = ((n_test + local_size - 1)//local_size)*local_size\n",
    "n_workgroups_train = global_size_train//local_size\n",
    "n_workgroups_test = global_size_test//local_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para poder hacer el padding lo que decidí fue copiar algunos datos al azar para poder completar los que faltan. Lo hacemos a continuación y vemos los nuevos tamaños de los conjuntos de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ePEdXgXOkM9J",
    "outputId": "c9319f61-e1e2-41ef-a3d3-f79bbcfb101c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Los 60032 datos de entrenamiento tienen dimensión 784 y dtype float64\n",
      "Los 10048 datos de entrenamiento tienen dimensión 784 y dtype float64\n"
     ]
    }
   ],
   "source": [
    "random_indices_train = np.random.choice(n_train, global_size_train - n_train, replace=True)\n",
    "random_indices_test = np.random.choice(n_test, global_size_test - n_test, replace=True)\n",
    "\n",
    "train_features = np.hstack([train_features, train_features[:, random_indices_train]])\n",
    "train_labels = np.hstack([train_labels, train_labels[:, random_indices_train]])\n",
    "\n",
    "test_features = np.hstack([test_features, test_features[:, random_indices_test]])\n",
    "test_labels = np.hstack([test_labels, test_labels[:, random_indices_test]])\n",
    "\n",
    "dim, n_train = train_features.shape\n",
    "print(\"Los\", n_train, \"datos de entrenamiento tienen dimensión\", dim, \"y dtype\", train_features.dtype)\n",
    "\n",
    "dim, n_test = test_features.shape\n",
    "print(\"Los\", n_test, \"datos de entrenamiento tienen dimensión\", dim, \"y dtype\", test_features.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver que ahora si son múltiplos de 64. A continuación vemos la implementación de la regresión logística en numpy, esta fue encontrada en el material adjunto en la tarea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "WwRj3Py1K0lE"
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1.0/(1.0+np.exp(-z))\n",
    "\n",
    "def forward_propagation(x, w, b):\n",
    "    z = w.T @ x + b\n",
    "    a = sigmoid(z)\n",
    "    return a\n",
    "\n",
    "def backward_propagation(x, a, y):\n",
    "    m = x.shape[1]\n",
    "    j = -np.sum(y * np.log(a) + (1-y) * np.log(1-a)) / m\n",
    "    dz = a - y\n",
    "    dw = (x @ dz.T) / m\n",
    "    db = np.mean(dz)\n",
    "    return dw, db, j\n",
    "\n",
    "def update(w, b, dw, db, lr):\n",
    "    w -= lr * dw\n",
    "    b -= lr * db\n",
    "    return w, b\n",
    "\n",
    "def train(x, y, w, b, lr, n_iter):\n",
    "    costs = []\n",
    "    for k in range(n_iter):\n",
    "        a = forward_propagation(x, w, b)\n",
    "        dw, db, j = backward_propagation(x, a, y)\n",
    "        w, b = update(w, b, dw, db, lr)\n",
    "        costs.append(j)\n",
    "    return w, b, costs\n",
    "\n",
    "def predict(x, w, b):\n",
    "    a = forward_propagation(x, w, b)\n",
    "    return np.round(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Establecemos los parámetros del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "3gMfNtKAKvMH"
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "initial_weights = np.zeros([dim, 1], dtype=float)\n",
    "initial_bias = 0.0\n",
    "n_iterations = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y lo entrenamos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kDhxppiZK5dF",
    "outputId": "9505da00-30e1-4d69-adfe-7dd4978764cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se tardó 21.43 segundos en entrenar\n"
     ]
    }
   ],
   "source": [
    "time_start = time.time()\n",
    "weights, bias, _ = train(train_features, train_labels, initial_weights, initial_bias, learning_rate, n_iterations)\n",
    "print(f\"Se tardó {time.time() - time_start:.2f} segundos en entrenar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notamos que se demoró 21 segundos, ahora veamos si está funcionando correctamente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k5p8MPh3LGOg",
    "outputId": "86d40a81-a5dd-4b44-c5c6-537d0b257e62"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se tardó 0.01 segundos en predecir\n",
      "Predichas 51341 datos correctamente de los 60032 datos de entrenamiento: 85.52272121535182 %\n",
      "Predichas 8629 datos correctamente de los 10048 datos de testeo: 85.87778662420382 %\n"
     ]
    }
   ],
   "source": [
    "time_start = time.time()\n",
    "\n",
    "prediction_train = predict(train_features, weights, bias)\n",
    "prediction_test = predict(test_features, weights, bias)\n",
    "\n",
    "print(f\"Se tardó {time.time() - time_start:.2f} segundos en predecir\")\n",
    "\n",
    "correct_train = np.sum(prediction_train == train_labels)\n",
    "correct_test = np.sum(prediction_test == test_labels)\n",
    "\n",
    "print(\"Predichas\", correct_train, \"datos correctamente de los\", n_train, \"datos de entrenamiento:\", correct_train/n_train*100, \"%\")\n",
    "print(\"Predichas\", correct_test, \"datos correctamente de los\", n_test, \"datos de testeo:\", correct_test/n_test*100, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predijo una cantidad razonable de datos correctamente, así que concluímos que funciona correctamente. Además notamos que la predicción fue prácticamente instantanea. Ahora vamos a implementar la regresión en OpenPyCL, lo primero será ver los dispositivos que tenemos disponibles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sGtWVd4TkjgH",
    "outputId": "7188c6a9-5e3b-461e-a842-acc2952b2498"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plataforma: NVIDIA CUDA\n",
      "    Dispositivo: NVIDIA A100-SXM4-40GB\n",
      "    Tipo de dispositivo: ALL | GPU\n",
      "Plataforma: Portable Computing Language\n",
      "    Dispositivo: pthread-Intel(R) Xeon(R) CPU @ 2.20GHz\n",
      "    Tipo de dispositivo: ALL | CPU\n"
     ]
    }
   ],
   "source": [
    "for platform in cl.get_platforms():\n",
    "    print(f\"Plataforma: {platform.name}\")\n",
    "    for device in platform.get_devices():\n",
    "        print(f\"    Dispositivo: {device.name}\")\n",
    "        print(f\"    Tipo de dispositivo: {cl.device_type.to_string(device.type)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que tenemos una GPU y una CPU disponibles. Notemos que tengo la GPU y CPU pagadas de colab porque se me acabó el cómputo y me quedaba poco para términas así que me compré unidades de cómputo XD. Seleccionemos la GPU primero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RRb3RYIQxgSE",
    "outputId": "5aed74a3-a4ce-4850-912b-4cc4e7f06f0f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre plataforma: NVIDIA CUDA\n",
      "Nombre dispositivo: NVIDIA A100-SXM4-40GB\n",
      "Tipo de dispositivo: ALL | GPU\n",
      "Maximo tamaño de grupo de trabajo: 1024\n"
     ]
    }
   ],
   "source": [
    "platform = cl.get_platforms()[0]\n",
    "device = platform.get_devices()[0]\n",
    "print(\"Nombre plataforma:\", platform.name)\n",
    "print(\"Nombre dispositivo:\", device.name)\n",
    "print(f\"Tipo de dispositivo: {cl.device_type.to_string(device.type)}\")\n",
    "print(\"Maximo tamaño de grupo de trabajo:\", device.max_work_group_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver que la hemos seleccionado correctamente y podemos ver sus características. Ahora creemos el contexto y la cola de trabajo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "w3NLvhEVxrR_"
   },
   "outputs": [],
   "source": [
    "ctx = cl.create_some_context()\n",
    "queue = cl.CommandQueue(ctx, properties=cl.command_queue_properties.PROFILING_ENABLE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tal como se pide, implementé tres kernels separados para las tareas de propagación hacia adelante (`forward`), retropropagación (`backward`), y predicción (`predict`). A continuación, detallo el diseño SIMD de cada uno:\n",
    "\n",
    "## **`forward`**\n",
    "- **Descripción del diseño SIMD**:  \n",
    "  Cada hilo procesa un dato en paralelo, realizando las siguientes operaciones:\n",
    "  1. Calcula el producto punto entre el vector de pesos $W$ y una instancia de los datos de entrada $X$.\n",
    "  2. Suma el sesgo $b$ al resultado del producto punto.\n",
    "  3. Aplica la función sigmoide $\\sigma(z) = \\frac{1}{1 + e^{-z}}$ para obtener la salida activada.\n",
    "\n",
    "- **Paralelismo**:  \n",
    "  El procesamiento paralelo se asegura asignando a cada hilo la tarea de procesar una instancia específica del conjunto de datos $X$. Esto permite realizar el cálculo de manera independiente para cada dato.\n",
    "\n",
    "\n",
    "## **`backward`**\n",
    "- **Descripción del diseño SIMD**:  \n",
    "  Este kernel se encarga de calcular los gradientes parciales para el sesgo $b$ y los pesos $W$. Para cada grupo de hilos se hace lo siguiente:\n",
    "  1. Un hilo representante acumula las contribuciones de los gradientes generadas por los demás hilos del grupo.\n",
    "  2. Los cálculos incluyen:\n",
    "     - La diferencia $dz = a - y$, lo cual es la contribución al gradiente de $b$\n",
    "     - La contribución del gradiente de $W$ como un producto punto entre $dz$ y las características de entrada $X$.\n",
    "  3. Finalmente, la suma acumulada de los gradientes del grupo se combina en la CPU mediante una reducción global (tal como se vio en ayudantía)\n",
    "\n",
    "- **Consideraciones de implementación**:  \n",
    "  - El paralelismo se da entre los grupos, ya que los grupos pueden realizar sus operaciones en paralelo (tal como se vio en los códigos entregados en ayudantía)\n",
    "  - La acumulación de gradientes dentro de un grupo se realiza utilizando un ciclo `for` en lugar de una suma en forma de árbol binario. Esto se debe a que en la GPU estándar esta aproximación resultó más rápida. En el caso de la GPU de pago ambas aproximaciones tienen tiempos similares.  \n",
    "  - La implementación de suma en forma de árbol binario está disponible al final del notebook.\n",
    "\n",
    "\n",
    "## **`predict`**\n",
    "- **Descripción del diseño SIMD**:  \n",
    "  Cada hilo realiza predicciones de manera independiente para una instancia de entrada, llevando a cabo las siguientes operaciones:\n",
    "  1. Calcula el producto punto entre $W$ y la instancia correspondiente de $X$.\n",
    "  2. Suma el término de sesgo $b$.\n",
    "  3. Aplica una función de umbral para determinar la clase predicha: $ y_{\\text{pred}} = 1.0 $ si $ z > 0 $, de lo contrario $ y_{\\text{pred}} = 0.0 $.\n",
    "\n",
    "- **Paralelismo**:  \n",
    "  Similar al kernel de propagación hacia adelante, cada hilo procesa una instancia diferente de manera independiente, garantizando el aprovechamiento del modelo SIMD.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "vD3JcZa3ZTqX"
   },
   "outputs": [],
   "source": [
    "kernel = \"\"\"\n",
    "__kernel void forward(\n",
    "    __global const double *W,\n",
    "    __global const double *X,\n",
    "    const double b,\n",
    "    __global double *a,\n",
    "    const int d)\n",
    "{\n",
    "    int global_id = get_global_id(0);\n",
    "\n",
    "    double z = b;\n",
    "    int offset = global_id*d;\n",
    "    for(int i = 0; i<d; i++){\n",
    "        z += W[i]*X[offset + i];\n",
    "    }\n",
    "\n",
    "    a[global_id] = 1.0/(1.0 + exp(-z));\n",
    "}\n",
    "\n",
    "__kernel void backward(\n",
    "    __global const double *a,\n",
    "    __global const double *X,\n",
    "    __global const double *y,\n",
    "    __global double *partial_b,\n",
    "    __global double *partial_w,\n",
    "    const int d,\n",
    "    const int m,\n",
    "    const int tot_groups)\n",
    "{\n",
    "    int group_size = get_local_size(0);\n",
    "    int local_id = get_local_id(0);\n",
    "    int group_id = get_group_id(0);\n",
    "    int global_id = get_global_id(0);\n",
    "\n",
    "    if(local_id == 0){\n",
    "        partial_b[group_id] = 0;\n",
    "        for(int j = 0; j<d; j++){\n",
    "            partial_w[j*tot_groups + group_id] = 0;\n",
    "        }\n",
    "        for(int i = 0; i<group_size; i++){\n",
    "            double dz = a[global_id + i] - y[global_id + i];\n",
    "            partial_b[group_id] += dz;\n",
    "            for(int j = 0; j<d; j++){\n",
    "                partial_w[j*tot_groups + group_id] += dz*X[(global_id + i)*d + j];\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "__kernel void predict(\n",
    "    __global const double *W,\n",
    "    __global const double *X,\n",
    "    const double b,\n",
    "    __global double *y_pred,\n",
    "    const int d)\n",
    "{\n",
    "    int global_id = get_global_id(0);\n",
    "\n",
    "    double z = b;\n",
    "    int offset = global_id*d;\n",
    "    for(int i = 0; i<d; i++){\n",
    "        z += W[i]*X[offset + i];\n",
    "    }\n",
    "\n",
    "    y_pred[global_id] = (z > 0) * 1.0;\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compilamos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "zT84RwV8nBUP"
   },
   "outputs": [],
   "source": [
    "prg = cl.Program(ctx, kernel).build()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que ha compilado correctamente, así que creamos los arreglos necesarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "_jye35gsnCDo"
   },
   "outputs": [],
   "source": [
    "db_train_labels = train_labels.astype(np.float64).squeeze()\n",
    "\n",
    "cl_Xtrain = cl_array.to_device(queue, train_features.T)\n",
    "cl_ytrain = cl_array.to_device(queue, db_train_labels)\n",
    "cl_Xtest = cl_array.to_device(queue, test_features.T)\n",
    "\n",
    "cl_W = cl_array.empty(queue, (dim,), dtype=np.float64)\n",
    "cl_b = cl_array.empty(queue, (1,), dtype=np.float64)\n",
    "\n",
    "cl_Wgrad = cl_array.empty(queue, (dim*n_workgroups_train,), dtype=np.float64)\n",
    "cl_bgrad = cl_array.empty(queue, (n_workgroups_train,), dtype=np.float64)\n",
    "cl_Wgradpartial = cl_array.empty(queue, (dim*n_workgroups_train,), dtype=np.float64)\n",
    "cl_bgradpartial = cl_array.empty(queue, (n_workgroups_train,), dtype=np.float64)\n",
    "cl_a = cl_array.empty(queue, (n_train,), dtype=np.float64)\n",
    "\n",
    "cl_predtrain = cl_array.empty(queue, (n_train,), dtype=np.float64)\n",
    "cl_predtest = cl_array.empty(queue, (n_test,), dtype=np.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y finalmente entrenamos el modelo usando PyOpenCL en la GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qyWeTFvhxyov",
    "outputId": "4b9ade35-165c-4d28-bb6f-d003df8f8f02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se tardó 20.50 segundos en entrenar, de los cuales 17.39 fueron en el kernel\n"
     ]
    }
   ],
   "source": [
    "time_start = time.time()\n",
    "time_kernel = 0\n",
    "\n",
    "for _ in range(n_iterations):\n",
    "\n",
    "    event = prg.forward(queue, (global_size_train,), (local_size, ), cl_W.data, cl_Xtrain.data, cl_b.data, cl_a.data, np.int32(dim))\n",
    "    event.wait()\n",
    "    time_kernel += 1e-9 *(event.profile.end - event.profile.start)\n",
    "    event = prg.backward(queue, (global_size_train,), (local_size, ), cl_a.data, cl_Xtrain.data, cl_ytrain.data, cl_bgrad.data, cl_Wgrad.data, np.int32(dim), np.int32(n_train), np.int32(n_workgroups_train))\n",
    "    event.wait()\n",
    "    time_kernel += 1e-9 *(event.profile.end - event.profile.start)\n",
    "\n",
    "    bgrad = float(sum(cl_bgrad.get()))\n",
    "    Wgrad = np.add.reduceat(cl_Wgrad.get(), np.arange(0, dim*n_workgroups_train, n_workgroups_train))\n",
    "\n",
    "    cl_b.set(cl_b.get() - (learning_rate/n_train) * bgrad)\n",
    "    cl_W.set(cl_W.get() - (learning_rate/n_train) * Wgrad)\n",
    "\n",
    "print(f\"Se tardó {time.time() - time_start:.2f} segundos en entrenar, de los cuales {time_kernel:.2f} fueron en el kernel\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que se demoró parecido a la implementación en numpy, y la mayor parte fueron cómputos en el kernel. En el entorno gratis esta se demoraba como la mitad de la implementación en numpy (esta como 45s y la otra como 80s), pero ahora parece que anduvieron igualados. Hagamos las predicciones y veamos el error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QiAcsoQ2g9p5",
    "outputId": "d9850a54-962e-4f99-bb51-4eafdc2335c5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se tardó 0.00 segundos en predecir, de los cuales 0.00 fueron en el kernel\n",
      "Predichas 51253 datos correctamente de los 60032 datos de entrenamiento: 85.37613272921108 %\n",
      "Predichas 8610 datos correctamente de los 10048 datos de testeo: 85.68869426751591 %\n"
     ]
    }
   ],
   "source": [
    "time_start = time.time()\n",
    "time_kernel = 0\n",
    "event = prg.predict(queue, (global_size_train,), (local_size,), cl_W.data, cl_Xtrain.data, cl_b.data, cl_predtrain.data, np.int32(dim))\n",
    "event.wait()\n",
    "time_kernel += 1e-9 *(event.profile.end - event.profile.start)\n",
    "predtrain = cl_predtrain.get()\n",
    "event = prg.predict(queue, (global_size_test,), (local_size,), cl_W.data, cl_Xtest.data, cl_b.data, cl_predtest.data, np.int32(dim))\n",
    "event.wait()\n",
    "time_kernel += 1e-9 *(event.profile.end - event.profile.start)\n",
    "predtest = cl_predtest.get()\n",
    "print(f\"Se tardó {time.time() - time_start:.2f} segundos en predecir, de los cuales {time_kernel:.2f} fueron en el kernel\")\n",
    "\n",
    "correct_train = np.sum(predtrain == train_labels)\n",
    "correct_test = np.sum(predtest == test_labels)\n",
    "\n",
    "print(\"Predichas\", correct_train, \"datos correctamente de los\", n_train, \"datos de entrenamiento:\", correct_train/n_train*100, \"%\")\n",
    "print(\"Predichas\", correct_test, \"datos correctamente de los\", n_test, \"datos de testeo:\", correct_test/n_test*100, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notamos que obtenemos una buena precisión por lo que el modelo está funcionando correctamente. Además observamos que las predicciones fueron prácticamente instantaneas. Ahora veamos como anda en la CPU, lo primero es que vamos a crear el entorno, lo haremos todo en este bloque de código ya que es clásico a estas alturas (y el kernel obviamente será el mismo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q-j8Wy7qJ-m3",
    "outputId": "e96db72a-6832-4cfa-fa9a-77c5d319c161"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre plataforma: Portable Computing Language\n",
      "Nombre dispositivo: pthread-Intel(R) Xeon(R) CPU @ 2.20GHz\n",
      "Tipo de dispositivo: ALL | CPU\n",
      "Maximo tamaño de grupo de trabajo: 4096\n"
     ]
    }
   ],
   "source": [
    "platform = cl.get_platforms()[1]\n",
    "device = platform.get_devices()[0]\n",
    "print(\"Nombre plataforma:\", platform.name)\n",
    "print(\"Nombre dispositivo:\", device.name)\n",
    "print(f\"Tipo de dispositivo: {cl.device_type.to_string(device.type)}\")\n",
    "print(\"Maximo tamaño de grupo de trabajo:\", device.max_work_group_size)\n",
    "ctx = cl.create_some_context()\n",
    "queue = cl.CommandQueue(ctx, properties=cl.command_queue_properties.PROFILING_ENABLE)\n",
    "prg = cl.Program(ctx, kernel).build()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notamos que se escogió correctamente la CPU, ahora creamos los arreglos que usaremos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "5SKk2fBkKHGm"
   },
   "outputs": [],
   "source": [
    "db_train_labels = train_labels.astype(np.float64).squeeze()\n",
    "\n",
    "cl_Xtrain = cl_array.to_device(queue, train_features.T)\n",
    "cl_ytrain = cl_array.to_device(queue, db_train_labels)\n",
    "cl_Xtest = cl_array.to_device(queue, test_features.T)\n",
    "\n",
    "cl_W = cl_array.empty(queue, (dim,), dtype=np.float64)\n",
    "cl_b = cl_array.empty(queue, (1,), dtype=np.float64)\n",
    "\n",
    "cl_Wgrad = cl_array.empty(queue, (dim*n_workgroups_train,), dtype=np.float64)\n",
    "cl_bgrad = cl_array.empty(queue, (n_workgroups_train,), dtype=np.float64)\n",
    "cl_Wgradpartial = cl_array.empty(queue, (dim*n_workgroups_train,), dtype=np.float64)\n",
    "cl_bgradpartial = cl_array.empty(queue, (n_workgroups_train,), dtype=np.float64)\n",
    "cl_a = cl_array.empty(queue, (n_train,), dtype=np.float64)\n",
    "\n",
    "cl_predtrain = cl_array.empty(queue, (n_train,), dtype=np.float64)\n",
    "cl_predtest = cl_array.empty(queue, (n_test,), dtype=np.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y ahora entrenamos el modelo en la CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S6poZHv4KJpo",
    "outputId": "a920864e-0fce-489f-ed86-8f9b602307cb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se tardó 20.50 segundos en entrenar, de los cuales 17.40 fueron en el kernel\n"
     ]
    }
   ],
   "source": [
    "time_start = time.time()\n",
    "time_kernel = 0\n",
    "\n",
    "for _ in range(n_iterations):\n",
    "\n",
    "    event = prg.forward(queue, (global_size_train,), (local_size, ), cl_W.data, cl_Xtrain.data, cl_b.data, cl_a.data, np.int32(dim))\n",
    "    event.wait()\n",
    "    time_kernel += 1e-9 *(event.profile.end - event.profile.start)\n",
    "    event = prg.backward(queue, (global_size_train,), (local_size, ), cl_a.data, cl_Xtrain.data, cl_ytrain.data, cl_bgrad.data, cl_Wgrad.data, np.int32(dim), np.int32(n_train), np.int32(n_workgroups_train))\n",
    "    event.wait()\n",
    "    time_kernel += 1e-9 *(event.profile.end - event.profile.start)\n",
    "\n",
    "    bgrad = float(sum(cl_bgrad.get()))\n",
    "    Wgrad = np.add.reduceat(cl_Wgrad.get(), np.arange(0, dim*n_workgroups_train, n_workgroups_train))\n",
    "\n",
    "    cl_b.set(cl_b.get() - (learning_rate/n_train) * bgrad)\n",
    "    cl_W.set(cl_W.get() - (learning_rate/n_train) * Wgrad)\n",
    "\n",
    "print(f\"Se tardó {time.time() - time_start:.2f} segundos en entrenar, de los cuales {time_kernel:.2f} fueron en el kernel\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notamos que se demoró similar a la GPU y a numpy, se siente un poco raro ya que en el entorno gratis andaba mejor que numpy y un poco peor que la GPU. Además notamos que la mayor parte del tiempo fue trabajo en el kernel. Ahora veamos la precisión del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZqGAct46KMc8",
    "outputId": "db5b85d7-ea59-430d-b760-04f49cbfd4a2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se tardó 0.00 segundos en predecir, de los cuales 0.00 fueron en el kernel\n",
      "Predichas 51253 datos correctamente de los 60032 datos de entrenamiento: 85.37613272921108 %\n",
      "Predichas 8610 datos correctamente de los 10048 datos de testeo: 85.68869426751591 %\n"
     ]
    }
   ],
   "source": [
    "time_start = time.time()\n",
    "time_kernel = 0\n",
    "event = prg.predict(queue, (global_size_train,), (local_size,), cl_W.data, cl_Xtrain.data, cl_b.data, cl_predtrain.data, np.int32(dim))\n",
    "event.wait()\n",
    "time_kernel += 1e-9 *(event.profile.end - event.profile.start)\n",
    "predtrain = cl_predtrain.get()\n",
    "event = prg.predict(queue, (global_size_test,), (local_size,), cl_W.data, cl_Xtest.data, cl_b.data, cl_predtest.data, np.int32(dim))\n",
    "event.wait()\n",
    "time_kernel += 1e-9 *(event.profile.end - event.profile.start)\n",
    "predtest = cl_predtest.get()\n",
    "print(f\"Se tardó {time.time() - time_start:.2f} segundos en predecir, de los cuales {time_kernel:.2f} fueron en el kernel\")\n",
    "\n",
    "correct_train = np.sum(predtrain == train_labels)\n",
    "correct_test = np.sum(predtest == test_labels)\n",
    "\n",
    "print(\"Predichas\", correct_train, \"datos correctamente de los\", n_train, \"datos de entrenamiento:\", correct_train/n_train*100, \"%\")\n",
    "print(\"Predichas\", correct_test, \"datos correctamente de los\", n_test, \"datos de testeo:\", correct_test/n_test*100, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notamos que obtuvimos la misma precisión que antes, por lo que el modelo funcionó correctamente. Además nuevamente la predicción fue casi instantanea"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xf-slsIGFrGE"
   },
   "source": [
    "Por último, dejo la implementación del backward en donde se hace la reducción en forma de árbol binario. En el entorno gratis esto se demoraba como el doble, pero en el entorno de pago esto se demoró prácticamente lo mismo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q4pS5BzGBkyR",
    "outputId": "ba04d4ff-d54e-4c5c-8ba6-f56a6748f299"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se tardó 20.53 segundos\n"
     ]
    }
   ],
   "source": [
    "platform = cl.get_platforms()[0]\n",
    "device = platform.get_devices()[0]\n",
    "\n",
    "ctx = cl.create_some_context()\n",
    "queue = cl.CommandQueue(ctx, properties=cl.command_queue_properties.PROFILING_ENABLE)\n",
    "\n",
    "kernel = \"\"\"\n",
    "__kernel void forward(\n",
    "    __global const double *W,\n",
    "    __global const double *X,\n",
    "    const double b,\n",
    "    __global double *a,\n",
    "    const int d)\n",
    "{\n",
    "    int global_id = get_global_id(0);\n",
    "\n",
    "    double z = b;\n",
    "    int offset = global_id*d;\n",
    "    for(int i = 0; i<d; i++){\n",
    "        z += W[i]*X[offset + i];\n",
    "    }\n",
    "\n",
    "    a[global_id] = 1.0/(1.0 + exp(-z));\n",
    "}\n",
    "\n",
    "__kernel void backward(\n",
    "    __global const double *a,\n",
    "    __global const double *X,\n",
    "    __global const double *y,\n",
    "    __global double *grad_b,\n",
    "    __global double *grad_w,\n",
    "    __global double *grad_bpartial,\n",
    "    __global double *grad_wpartial,\n",
    "    const int d,\n",
    "    const int m,\n",
    "    const int tot_groups)\n",
    "{\n",
    "    int group_size = get_local_size(0);\n",
    "    int local_id = get_local_id(0);\n",
    "    int group_id = get_group_id(0);\n",
    "    int global_id = get_global_id(0);\n",
    "\n",
    "    double dz = a[global_id] - y[global_id];\n",
    "    int offset = global_id*d;\n",
    "\n",
    "    grad_b[global_id] = dz;\n",
    "    for(int j = 0; j<d; j++){\n",
    "        grad_w[offset + j] += dz*X[offset + j];\n",
    "    }\n",
    "\n",
    "    int step = 2;\n",
    "    while(step <= group_size){\n",
    "        if(local_id%step == 0){\n",
    "            grad_b[global_id] += grad_b[global_id + step/2];\n",
    "            for(int j = 0; j<d; j++){\n",
    "                grad_w[offset + j] += dz*X[(global_id + step/2)*d + j];\n",
    "            }\n",
    "        }\n",
    "        barrier(CLK_GLOBAL_MEM_FENCE);\n",
    "        step *= 2;\n",
    "    }\n",
    "\n",
    "    if(local_id == 0){\n",
    "        grad_bpartial[group_id] = grad_b[global_id];\n",
    "        for(int j = 0; j<d; j++){\n",
    "            grad_wpartial[j*tot_groups + group_id] = grad_w[offset + j];\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "__kernel void predict(\n",
    "    __global const double *W,\n",
    "    __global const double *X,\n",
    "    const double b,\n",
    "    __global double *y_pred,\n",
    "    const int d)\n",
    "{\n",
    "    int global_id = get_global_id(0);\n",
    "\n",
    "    double z = b;\n",
    "    int offset = global_id*d;\n",
    "    for(int i = 0; i<d; i++){\n",
    "        z += W[i]*X[offset + i];\n",
    "    }\n",
    "\n",
    "    y_pred[global_id] = (z > 0) * 1.0;\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "prg = cl.Program(ctx, kernel).build()\n",
    "\n",
    "db_train_labels = train_labels.astype(np.float64).squeeze()\n",
    "\n",
    "cl_Xtrain = cl_array.to_device(queue, train_features.T)\n",
    "cl_ytrain = cl_array.to_device(queue, db_train_labels)\n",
    "cl_Xtest = cl_array.to_device(queue, test_features.T)\n",
    "\n",
    "cl_W = cl_array.empty(queue, (dim,), dtype=np.float64)\n",
    "cl_b = cl_array.empty(queue, (1,), dtype=np.float64)\n",
    "\n",
    "cl_Wgrad = cl_array.empty(queue, (dim*n_train,), dtype=np.float64)\n",
    "cl_bgrad = cl_array.empty(queue, (n_train,), dtype=np.float64)\n",
    "cl_Wgradpartial = cl_array.empty(queue, (dim*n_workgroups_train,), dtype=np.float64)\n",
    "cl_bgradpartial = cl_array.empty(queue, (n_workgroups_train,), dtype=np.float64)\n",
    "cl_a = cl_array.empty(queue, (n_train,), dtype=np.float64)\n",
    "\n",
    "cl_predtrain = cl_array.empty(queue, (n_train,), dtype=np.float64)\n",
    "cl_predtest = cl_array.empty(queue, (n_test,), dtype=np.float64)\n",
    "\n",
    "time_start = time.time()\n",
    "\n",
    "for _ in range(n_iterations):\n",
    "\n",
    "    prg.forward(queue, (global_size_train,), (local_size, ), cl_W.data, cl_Xtrain.data, cl_b.data, cl_a.data, np.int32(dim))\n",
    "    prg.backward(queue, (global_size_train,), (local_size, ), cl_a.data, cl_Xtrain.data, cl_ytrain.data, cl_bgrad.data, cl_Wgrad.data, cl_bgradpartial.data, cl_Wgradpartial.data, np.int32(dim), np.int32(n_train), np.int32(n_workgroups_train))\n",
    "\n",
    "    bgrad = float(sum(cl_bgradpartial.get()))\n",
    "    Wgrad = np.add.reduceat(cl_Wgradpartial.get(), np.arange(0, dim*n_workgroups_train, n_workgroups_train))\n",
    "    cl_b.set(cl_b.get() - (learning_rate/n_train) * bgrad)\n",
    "    cl_W.set(cl_W.get() - (learning_rate/n_train) * Wgrad)\n",
    "\n",
    "prg.predict(queue, (global_size_train,), (local_size,), cl_W.data, cl_Xtrain.data, cl_b.data, cl_predtrain.data, np.int32(dim))\n",
    "predtrain = cl_predtrain.get()\n",
    "prg.predict(queue, (global_size_test,), (local_size,), cl_W.data, cl_Xtest.data, cl_b.data, cl_predtest.data, np.int32(dim))\n",
    "predtest = cl_predtest.get()\n",
    "\n",
    "print(f\"Se tardó {time.time() - time_start:.2f} segundos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que se demoró unos 20 segundos tal como los anteriores"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "09fe9bac33f74916a02d6d5129165473": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1cc0858e19184a5f8e9efeacd3afe000": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "25ad33f379c24beb88cf56b6572d949d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "522f58dee3fb4f87b8ae0dfd3b375a35": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "774f65b2418e42fc841a672d1fe5b524": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "80e40d97a7f141e8aca43a544c0fc3b3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_09fe9bac33f74916a02d6d5129165473",
      "max": 5,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_1cc0858e19184a5f8e9efeacd3afe000",
      "value": 5
     }
    },
    "a6684a8c7093413480b76fb315972469": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_25ad33f379c24beb88cf56b6572d949d",
      "placeholder": "​",
      "style": "IPY_MODEL_cbd8edc90543413ebd5014100cab8134",
      "value": " 5/5 [00:00&lt;00:00, 24.14 file/s]"
     }
    },
    "cbd8edc90543413ebd5014100cab8134": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "cd3acb1a6cfa4d888de1f0c594a7016a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e939c1dc9f4648b78dcdaac48dc33fb0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f6fc955f1436400db5e33ea64f5a0ffd",
       "IPY_MODEL_80e40d97a7f141e8aca43a544c0fc3b3",
       "IPY_MODEL_a6684a8c7093413480b76fb315972469"
      ],
      "layout": "IPY_MODEL_774f65b2418e42fc841a672d1fe5b524"
     }
    },
    "f6fc955f1436400db5e33ea64f5a0ffd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cd3acb1a6cfa4d888de1f0c594a7016a",
      "placeholder": "​",
      "style": "IPY_MODEL_522f58dee3fb4f87b8ae0dfd3b375a35",
      "value": "Dl Completed...: 100%"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
